# model_loader.py
import torch
import json
import tensorflow as tf
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification,
    RobertaTokenizerFast,
    AutoTokenizer
)
from seqeval.metrics import classification_report
import sys
import codecs
import argparse
import numpy as np
import os
import re
import string


class ClassificationModel:
    """M√¥ h√¨nh ph√¢n lo·∫°i b√†i to√°n s·ª≠ d·ª•ng PhoBERT"""
    
    LABEL_NAMES = ["basic", "basic_word", "ownership", "ratio", "comparison"]

    def __init__(self, model_path):
        """Kh·ªüi t·∫°o model ph√¢n lo·∫°i b√†i to√°n

        Args:
            model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a model
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base", use_fast=False)

        # Prefer loading a TensorFlow SavedModel if present (training script saves Keras SavedModel),
        # otherwise fallback to a HuggingFace transformers model (PyTorch)
        self.is_tf_model = False
        saved_model_dir = None
        # Common saved_model layout: <model_path>/saved_model/... (saved via model.save(saved_path))
        if os.path.isdir(os.path.join(model_path, "saved_model")):
            saved_model_dir = os.path.join(model_path, "saved_model")
        # Also accept if user passed directly the saved_model directory
        elif os.path.isdir(model_path) and (
            os.path.exists(os.path.join(model_path, "saved_model.pb")) or
            os.path.isdir(os.path.join(model_path, "variables"))
        ):
            saved_model_dir = model_path

        if saved_model_dir:
            try:
                # Load Keras SavedModel (TF) for inference
                # This returns a keras.Model with a callable signature matching the training inputs
                self.tf_model = tf.keras.models.load_model(saved_model_dir)
                self.is_tf_model = True
                self.tf_use_signature = False
            except Exception as e:
                # Try loading with low-level saved_model loader and use serving_default signature
                try:
                    loaded = tf.saved_model.load(saved_model_dir)
                    if hasattr(loaded, "signatures") and "serving_default" in loaded.signatures:
                        self.saved_signature = loaded.signatures["serving_default"]
                        self.is_tf_model = True
                        self.tf_use_signature = True
                    else:
                        raise RuntimeError("SavedModel has no serving_default signature")
                except Exception as e2:
                    raise RuntimeError(f"Failed to load TF SavedModel from {saved_model_dir}: {e}; fallback failed: {e2}") from e2
        else:
            # Fallback: try to load a transformers model (PyTorch)
            self.model = AutoModelForTokenClassification.from_pretrained(
                model_path,
                num_labels=len(self.LABEL_NAMES)
            ).to(self.device)
        
    def predict(self, text):
        """D·ª± ƒëo√°n lo·∫°i b√†i to√°n t·ª´ vƒÉn b·∫£n
        
        Args:
            text: VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i

        Returns:
            tuple: (t√™n_nh√£n, dict x√°c su·∫•t t·ª´ng nh√£n)
        """
        # Tokenize v√† encode. Use TF tensors if using TF model, otherwise PyTorch tensors.
        if getattr(self, "is_tf_model", False):
            enc = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=96,
                return_tensors="tf",
            )

            # Build input dict matching training signature
            inputs = {"input_ids": enc["input_ids"], "attention_mask": enc["attention_mask"]}

            # If we loaded a Keras model, call it directly. Otherwise call the SavedModel signature.
            if getattr(self, "tf_use_signature", False):
                # signature expects TF Tensors; convert if needed
                sig_inputs = {k: tf.convert_to_tensor(v) for k, v in inputs.items()}
                sig_out = self.saved_signature(**sig_inputs)
                # signature returns a dict of outputs; take first tensor
                first_out = list(sig_out.values())[0]
                probs = first_out.numpy()[0]
            else:
                preds = self.tf_model(inputs, training=False)
                probs = preds.numpy()[0]
        else:
            inputs = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=96,
                return_tensors="pt"
            ).to(self.device)

            # D·ª± ƒëo√°n PyTorch HF model
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits[0]
                probs = torch.softmax(logits, dim=-1).cpu().numpy()

        pred_idx = int(np.argmax(probs))

        # Chuy·ªÉn v·ªÅ ƒë·ªãnh d·∫°ng d·ªÖ ƒë·ªçc
        prob_dict = {name: float(f"{p:.3f}") for name, p in zip(self.LABEL_NAMES, probs)}

        return self.LABEL_NAMES[pred_idx], prob_dict

    def classify_problem(self, text):
        """
        Ph∆∞∆°ng th·ª©c g·ªçi model ph√¢n lo·∫°i b√†i to√°n
        Args:
            text: VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i
        Returns:
            dict: K·∫øt qu·∫£ ph√¢n lo·∫°i v·ªõi lo·∫°i b√†i to√°n v√† x√°c su·∫•t
        """
        problem_type, probabilities = self.predict(text)
        return {
            "problem_type": problem_type,
            "probabilities": probabilities,
            "text": text
        }


class SeparateModel:
    def __init__(self, model_path, threshold=0.5):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.threshold = threshold
        self.label2id = {"CONT": 0, "BREAK": 1}
        self.id2label = {0: "CONT", 1: "BREAK"}

        self.tokenizer = RobertaTokenizerFast.from_pretrained(model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(
            model_path,
            id2label=self.id2label,
            label2id=self.label2id
        ).to(self.device)

    def separate_text(self, text):
        """T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u d·ª±a ho√†n to√†n v√†o model (kh√¥ng ti·ªÅn x·ª≠ l√Ω)."""
        words = text.split()
        if not words:
            return [text]

        # Encode & ch·∫°y model
        enc = self.tokenizer(words, is_split_into_words=True, return_tensors="pt", truncation=True, max_length=512)
        enc = {k: v.to(self.device) for k, v in enc.items()}

        with torch.no_grad():
            outputs = self.model(**enc)
            logits = outputs.logits[0]
            probs = torch.softmax(logits, dim=-1)

        sentences, cur = [], []
        enc_cpu = self.tokenizer(words, is_split_into_words=True, return_tensors="pt", truncation=True, max_length=512)
        word_ids_seq = enc_cpu.word_ids(batch_index=0)

        for i, w_id in enumerate(word_ids_seq):
            if w_id is None:
                continue
            is_first_sub = (i == 0) or (word_ids_seq[i-1] != w_id)
            if is_first_sub:
                cur.append(words[w_id])

            is_last_sub = (i + 1 == len(word_ids_seq)) or (word_ids_seq[i + 1] != w_id)
            if is_last_sub:
                prob_break = probs[i, self.label2id["BREAK"]].item()
                if prob_break >= self.threshold:
                    sentences.append(" ".join(cur).strip())
                    cur = []

        if cur:
            sentences.append(" ".join(cur).strip())

        # üßπ H·∫≠u x·ª≠ l√Ω: lo·∫°i b·ªè d·∫•u c√¢u ", . ? !" ·ªü cu·ªëi m·ªói c√¢u
        cleaned_sentences = [
            re.sub(r'[\s,\.?!]+$', '', s.strip()) for s in sentences if s.strip()
        ]

        return cleaned_sentences

    def split_sentences(self, text):
        """Ph∆∞∆°ng th·ª©c g·ªçi model t√°ch c√¢u"""
        sentences = self.separate_text(text)
        return {
            "original_text": text,
            "sentences": sentences,
            "sentence_count": len(sentences)
        }



class NERModel:
    def __init__(self, model_path, label_list):
        self.label_list = label_list
        self.id2label = {i: label for i, label in enumerate(label_list)}
        self.label2id = {label: i for i, label in enumerate(label_list)}
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load model v√† tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(
            model_path,
            id2label=self.id2label,
            label2id=self.label2id
        ).to(self.device)

    @staticmethod
    def strip_punctuation(s):
        """Lo·∫°i b·ªè d·∫•u c√¢u nh∆∞ng gi·ªØ l·∫°i c√°c to√°n t·ª≠ + - * / ="""
        # Gi·ªØ l·∫°i c√°c to√°n t·ª≠ v√† d·∫•u b·∫±ng
        keep_chars = "+-*/="
        # Lo·∫°i b·ªè c√°c d·∫•u c√¢u kh√°c
        no_punct = re.sub(r'[^\w\s' + re.escape(keep_chars) + ']', '', s)
        no_punct = re.sub(r'\s+', ' ', no_punct).strip()
        return no_punct

    def postprocess(self, tokens, labels):
        """
        H·∫≠u x·ª≠ l√Ω k·∫øt qu·∫£ NER:
        - Gh√©p c√°c token li√™n ti·∫øp c√πng nh√£n (n·∫øu c·∫ßn)
        - Lo·∫°i b·ªè nh√£n kh√¥ng h·ª£p l·ªá (n·∫øu c√≥)
        - Chu·∫©n h√≥a nh√£n (n·∫øu c·∫ßn)
        """
        processed_tokens = []
        processed_labels = []
        prev_label = None
        buffer = []
        for token, label in zip(tokens, labels):
            # V√≠ d·ª•: gh√©p c√°c token li√™n ti·∫øp c√πng nh√£n (B-*, I-*)
            if label.startswith("I-") and prev_label and prev_label[2:] == label[2:]:
                buffer.append(token)
            else:
                if buffer:
                    processed_tokens.append(" ".join(buffer))
                    processed_labels.append(prev_label)
                    buffer = []
                buffer = [token]
                prev_label = label
        if buffer:
            processed_tokens.append(" ".join(buffer))
            processed_labels.append(prev_label)
        # Lo·∫°i b·ªè nh√£n kh√¥ng h·ª£p l·ªá (v√≠ d·ª•: None)
        final_tokens = [t for t, l in zip(processed_tokens, processed_labels) if l is not None]
        final_labels = [l for l in processed_labels if l is not None]
        return final_tokens, final_labels

    def predict(self, text):
        """D·ª± ƒëo√°n nh√£n NER cho m·ªôt c√¢u (gi·ªØ nguy√™n to√†n b·ªô k√Ω t·ª± ƒë·∫ßu v√†o)."""
        words = text.split()

        tokens, word_ids = [], []
        for word_idx, word in enumerate(words):
            word_tokens = self.tokenizer.tokenize(word)
            tokens.extend(word_tokens)
            word_ids.extend([word_idx] * len(word_tokens))

        input_ids = [self.tokenizer.cls_token_id] + \
                    self.tokenizer.convert_tokens_to_ids(tokens) + \
                    [self.tokenizer.sep_token_id]
        attention_mask = [1] * len(input_ids)

        inputs = {
            "input_ids": torch.tensor([input_ids]).to(self.device),
            "attention_mask": torch.tensor([attention_mask]).to(self.device)
        }

        with torch.no_grad():
            outputs = self.model(**inputs)

        predictions = torch.argmax(outputs.logits, dim=2).cpu().numpy()[0][1:len(tokens) + 1]

        final_tokens, final_labels = [], []
        prev_word_idx = -1
        for token, word_idx, pred in zip(tokens, word_ids, predictions):
            if word_idx != prev_word_idx:
                final_tokens.append(token[2:] if token.startswith("##") else token)
                final_labels.append(self.id2label[pred])
                prev_word_idx = word_idx

        return final_tokens, final_labels

    def extract_entities(self, text):
        """
        Ph∆∞∆°ng th·ª©c g·ªçi model NER ƒë·ªÉ tr√≠ch xu·∫•t th·ª±c th·ªÉ
        Args:
            text: VƒÉn b·∫£n c·∫ßn tr√≠ch xu·∫•t
        Returns:
            dict: K·∫øt qu·∫£ tr√≠ch xu·∫•t v·ªõi tokens v√† labels
        """
        tokens, labels = self.predict(text)
        return {
            "text": text,
            "tokens": tokens,
            "labels": labels,
            "entities": list(zip(tokens, labels))
        }

    def evaluate(self, test_file_path, show_examples=3):
        """ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test ƒë·ªãnh d·∫°ng CoNLL"""

        def read_conll_file(file_path):
            tokens, labels = [], []
            current_tokens, current_labels = [], []
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        parts = line.split()
                        if len(parts) >= 2:
                            current_tokens.append(parts[0])
                            current_labels.append(parts[-1])
                    else:
                        if current_tokens:
                            tokens.append(current_tokens)
                            labels.append(current_labels)
                            current_tokens, current_labels = [], []
                if current_tokens:
                    tokens.append(current_tokens)
                    labels.append(current_labels)
            return tokens, labels

        test_tokens, true_labels = read_conll_file(test_file_path)
        all_predicted, all_true = [], []

        for i, (sentence_tokens, sentence_true) in enumerate(zip(test_tokens, true_labels)):
            text = " ".join(sentence_tokens)
            try:
                _, predicted = self.predict(text)

                min_len = min(len(predicted), len(sentence_true))
                all_predicted.append(predicted[:min_len])
                all_true.append(sentence_true[:min_len])

                if i < show_examples:
                    print(f"\nC√¢u {i + 1}:")
                    print("Token\t\tTrue\t\tPredicted")
                    print("-" * 40)
                    for j in range(min_len):
                        print(f"{sentence_tokens[j]}\t\t{sentence_true[j]}\t\t{predicted[j]}")
            except Exception as e:
                print(f"L·ªói x·ª≠ l√Ω c√¢u {i+1}: {e}")

        if all_predicted and all_true:
            print("\n" + "=" * 60)
            print("B√ÅO C√ÅO ƒê√ÅNH GI√Å")
            print("=" * 60)
            print(classification_report(all_true, all_predicted))
        else:
            print("Kh√¥ng c√≥ d·ª± ƒëo√°n n√†o ƒë·ªÉ ƒë√°nh gi√°.")


def setup_args():
    parser = argparse.ArgumentParser(description="Test Models")
    parser.add_argument("--ner_model_path", type=str, default="src/model/ner_model")
    parser.add_argument("--separate_model_path", type=str, default="src/model/sent_split_model")
    parser.add_argument("--classification_model_path", type=str, default="src/model/classification_model")
    parser.add_argument("--test_file", type=str, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file test CoNLL")
    parser.add_argument("--text", type=str, help="VƒÉn b·∫£n ƒë·ªÉ test")
    parser.add_argument("--output_file", type=str, help="Ghi k·∫øt qu·∫£ v√†o file")
    return parser.parse_args()


def setup_stdout(output_file=None):
    """Fix encoding stdout + redirect n·∫øu c√≥ file output"""
    if sys.stdout.encoding != "UTF-8":
        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")

    original_stdout = sys.stdout
    if output_file:
        sys.stdout = open(output_file, "w", encoding="utf-8")
    return original_stdout


def get_label_list():
    """Tr·∫£ v·ªÅ danh s√°ch nh√£n NER"""
    return [
        "O",
        "B-NUM", "I-NUM",
        "B-AGENT", "I-AGENT",
        "B-REL", "I-REL",
        "B-VALUE", "I-VALUE",
        "B-UNIT", "I-UNIT",
        "B-ATTRIBUTE", "I-ATTRIBUTE",
        "B-QUESTION", "I-QUESTION"
    ]


def run_test(args):
    # Kh·ªüi t·∫°o c√°c model
    label_list = get_label_list()
    ner_model = NERModel(args.ner_model_path, label_list)
    separate_model = SeparateModel(args.separate_model_path)
    classification_model = ClassificationModel(args.classification_model_path)
    
    print(f"ƒê√£ t·∫£i NER model t·ª´ {args.ner_model_path}")
    print(f"ƒê√£ t·∫£i Separate model t·ª´ {args.separate_model_path}")
    print(f"ƒê√£ t·∫£i Classification model t·ª´ {args.classification_model_path}")
    print(f"ƒêang s·ª≠ d·ª•ng device: {ner_model.device}")

    def analyze_text(text, is_sample=False):
        # S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c m·ªõi ƒë·ªÉ ph√¢n lo·∫°i b√†i to√°n
        classification_result = classification_model.classify_problem(text)
        print(f"\nK·∫æT QU·∫¢ PH√ÇN LO·∫†I{' (Sample)' if is_sample else ''}:")
        print("-" * 60)
        print(f"Lo·∫°i b√†i to√°n: {classification_result['problem_type']}")
        print("X√°c su·∫•t t·ª´ng lo·∫°i:")
        max_type_len = max(len(t) for t in classification_result['probabilities'].keys())
        for t, p in classification_result['probabilities'].items():
            print(f"  {t:<{max_type_len}} : {p:.3f}")

        # S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c m·ªõi ƒë·ªÉ t√°ch c√¢u
        separation_result = separate_model.split_sentences(text)
        
        # H√†m strip_punctuation m·ªõi ƒë·ªÉ gi·ªØ l·∫°i to√°n t·ª≠ khi hi·ªÉn th·ªã
        def strip_punctuation_keep_math(s):
            keep_chars = "+-*/="
            no_punct = re.sub(r'[^\w\s' + re.escape(keep_chars) + ']', '', s)
            no_punct = re.sub(r'\s+', ' ', no_punct).strip()
            return no_punct

        sentences = separation_result['sentences']


        print(f"\nK·∫æT QU·∫¢ T√ÅCH C√ÇU{' (Sample)' if is_sample else ''}:")
        print("-" * 60)
        print(f"S·ªë l∆∞·ª£ng c√¢u: {separation_result['sentence_count']}")
        for i, sentence in enumerate(sentences, 1):
            print(f"[{i}] {sentence}")

        print(f"\nK·∫æT QU·∫¢ PH√ÇN T√çCH NER{' (Sample)' if is_sample else ''}:")
        print("-" * 60)
        for i, sentence in enumerate(sentences, 1):
            # sentence ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch nh∆∞ng v·∫´n gi·ªØ to√°n t·ª≠
            clean_sentence = sentence
            print(f"\nC√¢u [{i}]: {clean_sentence}")
            print("-" * 40)
            
            # S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c m·ªõi ƒë·ªÉ tr√≠ch xu·∫•t th·ª±c th·ªÉ
            ner_result = ner_model.extract_entities(clean_sentence)
            
            # T√≠nh ƒë·ªô r·ªông c·ªôt hi·ªÉn th·ªã
            max_token_len = max(len(token) for token in ner_result['tokens']) if ner_result['tokens'] else 0
            token_col = max(max_token_len + 2, 10)
            label_col = 15
            total_width = token_col + label_col + 5
            
            # In header
            print("‚îå" + "‚îÄ" * token_col + "‚î¨" + "‚îÄ" * label_col + "‚îê")
            print(f"‚îÇ{'Token':<{token_col}}‚îÇ{'Nh√£n':<{label_col}}‚îÇ")
            print("‚îú" + "‚îÄ" * token_col + "‚îº" + "‚îÄ" * label_col + "‚î§")
            
            # In n·ªôi dung
            for token, label in ner_result['entities']:
                # B·ªè @@ ·ªü cu·ªëi token n·∫øu c√≥
                clean_token = token[:-2] if token.endswith("@@") else token
                print(f"‚îÇ{clean_token:<{token_col}}‚îÇ{label:<{label_col}}‚îÇ")
            
            print("‚îî" + "‚îÄ" * token_col + "‚î¥" + "‚îÄ" * label_col + "‚îò")

    if args.test_file:
        ner_model.evaluate(args.test_file)
    elif args.text:
        analyze_text(args.text)
    else:
        sample_text = "1.3 + 4 * 5 - 6 / 2 = ?"
        analyze_text(sample_text, is_sample=True)


def main():
    args = setup_args()
    original_stdout = setup_stdout(args.output_file)

    run_test(args)

    if args.output_file:
        sys.stdout.close()
        sys.stdout = original_stdout
        print(f"ƒê√£ ghi k·∫øt qu·∫£ v√†o {args.output_file}")


# Th√™m c√°c h√†m ti·ªán √≠ch ƒë·ªÉ s·ª≠ d·ª•ng t·ª´ b√™n ngo√†i
def create_classification_model(model_path):
    """T·∫°o v√† tr·∫£ v·ªÅ model ph√¢n lo·∫°i"""
    return ClassificationModel(model_path)


def create_separate_model(model_path, threshold=0.5):
    """T·∫°o v√† tr·∫£ v·ªÅ model t√°ch c√¢u"""
    return SeparateModel(model_path, threshold)


def create_ner_model(model_path, label_list=None):
    """T·∫°o v√† tr·∫£ v·ªÅ model NER"""
    if label_list is None:
        label_list = get_label_list()
    return NERModel(model_path, label_list)


if __name__ == "__main__":
    main()